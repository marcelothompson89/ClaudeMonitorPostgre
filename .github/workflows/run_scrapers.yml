name: Ejecutar Scrapers

on:
  schedule:
    # Ejecutar cada 6 horas - a las 0:00, 6:00, 12:00 y 18:00 UTC
    - cron: '0 0,6,12,18 * * *'
  workflow_dispatch:  # Permite ejecutar manualmente el flujo desde la interfaz de GitHub

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout código
        uses: actions/checkout@v2

      - name: Configurar Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.12'  # Puedes ajustar la versión según necesites

      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Crear directorio de management command
        run: |
          mkdir -p scrapers/management/commands
          touch scrapers/management/__init__.py
          touch scrapers/management/commands/__init__.py

      - name: Crear comando personalizado
        run: |
          echo 'from django.core.management.base import BaseCommand
          from scrapers.tasks import run_all_scrapers

          class Command(BaseCommand):
              help = "Ejecuta todos los scrapers disponibles"

              def handle(self, *args, **options):
                  results = run_all_scrapers()
                  self.stdout.write(
                      self.style.SUCCESS(f"Ejecución completada. Procesados {results[\"summary\"][\"total_processed\"]} items")
                  )' > scrapers/management/commands/run_scrapers.py

      - name: Ejecutar scrapers
        env:
          # Variables básicas de Django según tu settings.py
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          DEBUG: 'False'
          
          # Variables de base de datos
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
        run: |
          python manage.py run_scrapers