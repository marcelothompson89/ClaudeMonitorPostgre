name: Run Scrapers Every 6 Hours

on:
  schedule:
    - cron: '0 */6 * * *'  # Ejecutar cada 6 horas
  workflow_dispatch:  # Permite ejecutar manualmente

jobs:
  run-scrapers:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout código
      uses: actions/checkout@v3
      
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Instalar dependencias
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        playwright install chromium

    - name: Configurar variables de entorno
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
      run: |
        # Creamos un archivo .env para las variables de entorno
        echo "DATABASE_URL=$DATABASE_URL" > .env
        echo "SECRET_KEY=$DJANGO_SECRET_KEY" >> .env
        echo "DEBUG=False" >> .env
        
    - name: Crear archivo de configuración para GitHub Actions
      run: |
        # Creamos un archivo de configuración específico para GitHub Actions
        cat > github_actions_settings.py << EOF
        # Este archivo es generado automáticamente por GitHub Actions
        
        import os
        import sys
        import dj_database_url
        from pathlib import Path

        # Cargar variables desde .env
        from dotenv import load_dotenv
        load_dotenv()

        # Configuración básica
        BASE_DIR = Path(__file__).resolve().parent
        SECRET_KEY = os.environ.get('SECRET_KEY', '${{ secrets.DJANGO_SECRET_KEY }}')
        DEBUG = False

        # Configuración de la base de datos
        DATABASES = {
            'default': dj_database_url.config(
                default=os.environ.get('DATABASE_URL', '${{ secrets.DATABASE_URL }}'),
                conn_max_age=600,
                conn_health_checks=True,
            )
        }

        # Aplicaciones instaladas necesarias para los scrapers
        INSTALLED_APPS = [
            'django.contrib.auth',
            'django.contrib.contenttypes',
            'django.contrib.sessions',
            'django.contrib.messages',
            'django.contrib.staticfiles',
            'alertas',
            'scrapers',
        ]

        # Configuración mínima para ejecutar comandos
        MIDDLEWARE = [
            'django.middleware.security.SecurityMiddleware',
            'django.contrib.sessions.middleware.SessionMiddleware',
            'django.middleware.common.CommonMiddleware',
            'django.middleware.csrf.CsrfViewMiddleware',
            'django.contrib.auth.middleware.AuthenticationMiddleware',
            'django.contrib.messages.middleware.MessageMiddleware',
            'django.middleware.clickjacking.XFrameOptionsMiddleware',
        ]

        ROOT_URLCONF = 'alertas_project.urls'

        TEMPLATES = [
            {
                'BACKEND': 'django.template.backends.django.DjangoTemplates',
                'DIRS': [],
                'APP_DIRS': True,
                'OPTIONS': {
                    'context_processors': [
                        'django.template.context_processors.debug',
                        'django.template.context_processors.request',
                        'django.contrib.auth.context_processors.auth',
                        'django.contrib.messages.context_processors.messages',
                    ],
                },
            },
        ]

        WSGI_APPLICATION = 'alertas_project.wsgi.application'
        STATIC_URL = '/static/'
        DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'

        ALLOWED_HOSTS = ['*']
        EOF
        
    - name: Ejecutar scrapers
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
      run: |
        # Configuramos DJANGO_SETTINGS_MODULE para usar nuestro archivo personalizado
        export DJANGO_SETTINGS_MODULE=github_actions_settings
        # Añadimos el directorio actual al PYTHONPATH
        export PYTHONPATH=$PYTHONPATH:$(pwd)
        
        # Ejecutamos todos los scrapers y guardamos el resultado en un log
        python manage.py run_scrapers --json > scraper_log.json
        
    - name: Guardar log de ejecución como artefacto
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs
        path: scraper_log.json
        
    - name: Limpiar archivos temporales
      if: always()
      run: |
        rm -f .env github_actions_settings.py scraper_log.json