name: Ejecutar Scrapers

on:
  schedule:
    # Ejecutar cada 6 horas - a las 0:00, 6:00, 12:00 y 18:00 UTC
    - cron: '0 0,6,12,18 * * *'
  workflow_dispatch:  # Permite ejecutar manualmente el flujo desde la interfaz de GitHub

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout código
        uses: actions/checkout@v2

      - name: Configurar Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.12'  # Ajusta según necesites

      - name: Instalar dependencias y Playwright
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium

      - name: Crear directorio de management command (si no existe)
        run: |
          mkdir -p scrapers/management/commands
          touch scrapers/management/__init__.py
          touch scrapers/management/commands/__init__.py

      - name: Crear comando personalizado (si no existe)
        run: |
          if [ ! -f scrapers/management/commands/run_scrapers.py ]; then
            echo 'from django.core.management.base import BaseCommand
          from scrapers.tasks import run_all_scrapers

          class Command(BaseCommand):
              help = "Ejecuta todos los scrapers disponibles"

              def handle(self, *args, **options):
                  results = run_all_scrapers()
                  self.stdout.write(
                      self.style.SUCCESS(f"Ejecución completada. Procesados {results[\"summary\"][\"total_processed\"]} items")
                  )' > scrapers/management/commands/run_scrapers.py
          fi

      # Crear archivo .env con variables de entorno
      - name: Crear archivo .env
        run: |
          echo "EMAIL_HOST_USER=noreply@example.com" > .env
          echo "EMAIL_HOST_PASSWORD=placeholder" >> .env
          echo "SCRAPER_API_TOKEN=placeholder" >> .env
          # Añadir variables de base de datos para verificar conexión
          echo "DB_NAME=${{ secrets.DB_NAME }}" >> .env
          echo "DB_USER=${{ secrets.DB_USER }}" >> .env
          echo "DB_PASSWORD=${{ secrets.DB_PASSWORD }}" >> .env
          echo "DB_HOST=${{ secrets.DB_HOST }}" >> .env
          echo "DB_PORT=${{ secrets.DB_PORT }}" >> .env

      # Verificar conexión a base de datos antes de ejecutar scrapers
      - name: Verificar conexión a base de datos
        run: |
          python -c "
          import os
          import psycopg2
          import time
          
          max_attempts = 3
          attempt = 0
          
          while attempt < max_attempts:
              try:
                  conn = psycopg2.connect(
                      dbname=os.environ.get('DB_NAME'),
                      user=os.environ.get('DB_USER'),
                      password=os.environ.get('DB_PASSWORD'),
                      host=os.environ.get('DB_HOST'),
                      port=os.environ.get('DB_PORT')
                  )
                  conn.close()
                  print('Conexión a base de datos exitosa')
                  exit(0)
              except Exception as e:
                  attempt += 1
                  print(f'Intento {attempt}: Error conectando a base de datos: {e}')
                  if attempt < max_attempts:
                      print(f'Reintentando en 5 segundos...')
                      time.sleep(5)
                  else:
                      print('No se pudo conectar a la base de datos después de múltiples intentos')
                      exit(1)
          "

      - name: Ejecutar scrapers
        env:
          # Variables básicas de Django
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          DEBUG: 'False'
          
          # Variables de base de datos
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          
          # Valores para variables adicionales
          EMAIL_HOST_USER: 'noreply@example.com'
          EMAIL_HOST_PASSWORD: 'placeholder'
          SCRAPER_API_TOKEN: 'placeholder'
        run: |
          # Verificar que Playwright esté correctamente instalado
          playwright --version
          
          # Ejecutar scrapers
          python manage.py run_scrapers