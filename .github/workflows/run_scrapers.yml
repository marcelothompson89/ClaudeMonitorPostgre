name: Run Scrapers Every 6 Hours

on:
  schedule:
    - cron: '0 */6 * * *'  # Ejecutar cada 6 horas
  workflow_dispatch:  # Permite ejecutar manualmente

jobs:
  run-scrapers:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout código
      uses: actions/checkout@v3
      
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Instalar dependencias
      run: |
        python -m pip install --upgrade pip
        pip install requests python-dotenv asyncio
        pip install -r requirements.txt
        playwright install chromium
        
    - name: Crear script para ejecutar scrapers
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        cat > run_scrapers_standalone.py << EOF
        import os
        import json
        import sys
        import requests
        import asyncio
        import importlib
        from datetime import datetime
        
        # Definir directamente los scrapers disponibles aquí para evitar dependencias de Django
        AVAILABLE_SCRAPERS = {
            'argentina_anmat': {
                'module': 'scrapers.scrapers.anmat_noti_ar',
                'function': 'scrape_anmat_noti_ar',
                'name': 'ANMAT Argentina',
                'description': 'Obtiene noticias de ANMAT Argentina'
            },
            'mexico_animalpolitico': {
                'module': 'scrapers.scrapers.animalpolitico_mx',
                'function': 'scrape_animal_politico_salud',
                'name': 'Animal Político México',
                'description': 'Obtiene noticias del diario Animal Político México'
            },
            'chile_anamed': {
                'module': 'scrapers.scrapers.anamed_cl',
                'function': 'scrape_anamed_cl',
                'name': 'ANAMED Chile',
                'description': 'Obtiene alertas de ANAMED Chile'
            },
            'brasil_anvisa_normas': {
                'module': 'scrapers.scrapers.anvisa_normas_br',
                'function': 'scrape_anvisa_normas_br',
                'name': 'ANVISA Normas de Brasil',
                'description': 'Obtiene normas de ANVISA Brasil'
            },
            'brasil_anvisa_noticias': {
                'module': 'scrapers.scrapers.anvisa_noti_br',
                'function': 'scrape_anvisa_noti_br',
                'name': 'ANVISA Noticias de Brasil',
                'description': 'Obtiene noticias de ANVISA Brasil'
            },
            'argentina_boletinoficial': {
                'module': 'scrapers.scrapers.boletinoficial_ar',
                'function': 'scrape_boletin_oficial_ar',
                'name': 'Boletín Oficial de Argentina',
                'description': 'Obtiene avises de Boletín Oficial de Argentina'
            },
            'cepal_reg': {
                'module': 'scrapers.scrapers.cepal_reg',
                'function': 'scrape_cepal_noticias_reg',
                'name': 'CEPAL Regional',
                'description': 'Obtiene noticias de CEPAL Regional'
            },
            'mexico_cofepris_noticias': {
                'module': 'scrapers.scrapers.cofepris_noti_mx',
                'function': 'scrape_cofepris_noti_mx',
                'name': 'COFEPRIS Noticias de México',
                'description': 'Obtiene noticias de COFEPRIS México'
            },
            'peru_congreso_comunicaciones': {
                'module': 'scrapers.scrapers.congreso_comu_pe',
                'function': 'scrape_congreso_comu_pe',
                'name': 'Comunicaciones Congreso de Perú',
                'description': 'Obtiene Comunicaciones  del Congreso de Perú'
            }
            # Añadir más scrapers según sea necesario
        }
        
        # Configuración
        SUPABASE_URL = os.environ.get('SUPABASE_URL')
        SUPABASE_KEY = os.environ.get('SUPABASE_KEY')
        
        # Verificar que tenemos las credenciales
        if not SUPABASE_URL or not SUPABASE_KEY:
            print("Error: Faltan credenciales de Supabase")
            sys.exit(1)
        
        # Función para ejecutar un scraper específico
        async def run_scraper(scraper_id, scraper_info):
            try:
                # Importar dinámicamente el módulo y función del scraper
                module = importlib.import_module(scraper_info['module'])
                scraper_function = getattr(module, scraper_info['function'])
                
                # Ejecutar el scraper (asumiendo que son funciones asíncronas)
                print(f"Ejecutando scraper: {scraper_info['name']}")
                items = await scraper_function()
                
                return {
                    'id': scraper_id,
                    'name': scraper_info['name'],
                    'success': True,
                    'items': items,
                    'items_count': len(items),
                    'error': None
                }
            except Exception as e:
                error_msg = f"Error ejecutando scraper {scraper_id}: {str(e)}"
                print(error_msg)
                return {
                    'id': scraper_id,
                    'name': scraper_info.get('name', scraper_id),
                    'success': False,
                    'items': [],
                    'items_count': 0,
                    'error': error_msg
                }
        
        # Función para enviar datos a Supabase
        def send_to_supabase(items):
            if not items:
                print("No hay items para enviar")
                return True
                
            headers = {
                "apikey": SUPABASE_KEY,
                "Authorization": f"Bearer {SUPABASE_KEY}",
                "Content-Type": "application/json",
                "Prefer": "resolution=merge-duplicates"
            }
            
            url = f"{SUPABASE_URL}/rest/v1/alertas"
            
            # Preparar los datos para Supabase según el modelo Alerta
            processed_items = []
            for item in items:
                # Convertir fechas a formato ISO string si es necesario
                presentation_date = item.get("presentation_date")
                if presentation_date and not isinstance(presentation_date, str):
                    try:
                        presentation_date = presentation_date.isoformat()
                    except:
                        presentation_date = str(presentation_date)
                
                # Mapear los datos al modelo Alerta
                processed_item = {
                    "title": item.get("title", ""),
                    "description": item.get("description", ""),
                    "source_type": item.get("source_type", ""),
                    "category": item.get("category", ""),
                    "country": item.get("country", ""),
                    "source_url": item.get("source_url", ""),
                    "institution": item.get("institution", ""),
                    "presentation_date": presentation_date,
                }
                
                # Añadir campos de metadatos si existen
                if "metadata" in item and item["metadata"]:
                    if "image_url" in item["metadata"]:
                        processed_item["metadata_nota_url"] = item["metadata"].get("image_url")
                    if "publicacion_url" in item["metadata"]:
                        processed_item["metadata_publicacion_url"] = item["metadata"].get("publicacion_url")
                
                processed_items.append(processed_item)
            
            try:
                # Dividir en lotes si hay muchos items (para evitar payloads demasiado grandes)
                batch_size = 100
                success = True
                
                for i in range(0, len(processed_items), batch_size):
                    batch = processed_items[i:i + batch_size]
                    print(f"Enviando lote de {len(batch)} items a Supabase ({i+1}-{i+len(batch)} de {len(processed_items)})")
                    
                    response = requests.post(
                        url,
                        headers=headers,
                        json=batch
                    )
                    
                    if response.status_code not in (200, 201, 204):
                        print(f"Error enviando lote: {response.status_code} - {response.text}")
                        success = False
                
                return success
            except Exception as e:
                print(f"Error al enviar datos a Supabase: {str(e)}")
                return False
        
        # Función principal que ejecuta todos los scrapers
        async def run_all_scrapers():
            print(f"Iniciando ejecución de {len(AVAILABLE_SCRAPERS)} scrapers...")
            
            # Ejecutar scrapers en paralelo (con límite para no sobrecargar)
            results = []
            
            # Para evitar problemas, ejecutamos los scrapers en grupos de 3
            batch_size = 3
            scrapers_items = list(AVAILABLE_SCRAPERS.items())
            
            for i in range(0, len(scrapers_items), batch_size):
                batch = scrapers_items[i:i + batch_size]
                print(f"Ejecutando lote de {len(batch)} scrapers ({i+1}-{i+len(batch)} de {len(scrapers_items)})")
                
                # Crear tareas para este lote
                tasks = [run_scraper(scraper_id, scraper_info) for scraper_id, scraper_info in batch]
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)
            
            # Estadísticas
            success_count = sum(1 for r in results if r['success'])
            failure_count = sum(1 for r in results if not r['success'])
            total_items = sum(r['items_count'] for r in results)
            
            print(f"Ejecución de scrapers completada: {success_count} exitosos, {failure_count} fallidos")
            print(f"Total de items obtenidos: {total_items}")
            
            # Recopilar todos los items para enviar a Supabase
            all_items = []
            for result in results:
                if result['success'] and result['items']:
                    all_items.extend(result['items'])
            
            # Enviar todos los items a Supabase
            if all_items:
                print(f"Enviando {len(all_items)} items a Supabase...")
                supabase_success = send_to_supabase(all_items)
                if supabase_success:
                    print("Datos enviados con éxito a Supabase")
                else:
                    print("Error enviando datos a Supabase")
            else:
                print("No hay items para enviar a Supabase")
            
            # Crear informe
            report = {
                "timestamp": datetime.now().isoformat(),
                "scrapers_count": len(AVAILABLE_SCRAPERS),
                "success_count": success_count,
                "failure_count": failure_count,
                "total_items": total_items,
                "results": [
                    {
                        "id": r['id'],
                        "name": r['name'],
                        "success": r['success'],
                        "items_count": r['items_count'],
                        "error": r['error']
                    }
                    for r in results
                ]
            }
            
            # Guardar informe
            with open("scraper_log.json", "w") as f:
                json.dump(report, f, indent=2, default=str)
            
            print("Informe guardado en scraper_log.json")
            
            return success_count, failure_count
        
        # Punto de entrada
        if __name__ == "__main__":
            success, failures = asyncio.run(run_all_scrapers())
            sys.exit(0 if failures == 0 else 1)
        EOF
        
    - name: Ejecutar scrapers y enviar a Supabase
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        python run_scrapers_standalone.py
        
    - name: Guardar log de ejecución como artefacto
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs
        path: scraper_log.json
        retention-days: 5
        
    - name: Limpiar archivos temporales
      if: always()
      run: |
        rm -f run_scrapers_standalone.py