name: Run Scrapers Every 6 Hours

on:
  schedule:
    - cron: '0 */6 * * *'  # Ejecutar cada 6 horas (00:00, 06:00, 12:00, 18:00 UTC)
  workflow_dispatch:  # Permite ejecutar manualmente desde la interfaz de GitHub

jobs:
  run-scrapers:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout código
      uses: actions/checkout@v3
      
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Instalar dependencias
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        playwright install chromium
        
    - name: Configurar variables de entorno
      run: |
        # Configura todas las variables de entorno necesarias para Django y la base de datos
        echo "DATABASE_URL=${{ secrets.DATABASE_URL }}" >> $GITHUB_ENV
        echo "DJANGO_SECRET_KEY=${{ secrets.DJANGO_SECRET_KEY }}" >> $GITHUB_ENV
        # Añade otras variables de entorno que necesite tu aplicación
        
    - name: Ejecutar scrapers
      run: |
        # Usar directamente el comando de Django que ya tienes configurado
        python manage.py run_scrapers --json
        
    - name: Notificar resultado
      if: always()
      run: |
        echo "Scrapers ejecutados con el estado: ${{ job.status }}"